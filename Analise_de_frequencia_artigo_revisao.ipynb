{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3214e889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy  as np\n",
    "import pandas as pd\n",
    "import json\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "452ddc12",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.corpus\n",
    "from sklearn.feature_extraction.text  import TfidfVectorizer\n",
    "from sklearn.metrics                  import silhouette_samples, silhouette_score\n",
    "from sklearn.preprocessing            import normalize\n",
    "from nltk.tokenize                    import word_tokenize\n",
    "import re\n",
    "from unidecode                        import unidecode\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3f7a7516",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot  as plt\n",
    "import matplotlib.cm      as cm\n",
    "import seaborn            as sns\n",
    "from sklearn.metrics                  import silhouette_samples, silhouette_score\n",
    "from wordcloud                        import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "67d0c0e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "40001123",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTEXT_BASED_MACHINE_TRANSLATION = \"he current approaches for machine translation usually require large set of parallel corpus in order to achieve fluency like in the case of neural machine translation NMT, statistical machine translation SMT and example-based machinetranslation EBMT. The context awareness of phrase-based machine translation PBMT approaches is also questionable. This research develops a system thattranslates English text to Amharic text using a combination of context based machine translation CBMT and a recurrent neural network machine translation RNNMT. We built a bilingual dictionary for the CBMT system to use along witha large target corpus. The RNNMT model has then been provided with the output of the CBMT and a parallel corpus for training. Our combinational approachon English-Amharic language pair yields a performance improvement over thesimple neural machine translation NMT.\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "314bcf64",
   "metadata": {},
   "outputs": [],
   "source": [
    "Deep_Transfer_Learning_for_writer = \"In digital paleography, recent technology advance\u0002ments are used to support paleographers in the study andanalysis of ancient documents. One main goal of paleographersis to identify the different scribes (writers) who wrote a givenmanuscript. Deep learning has recently received much attentionfrom researchers as classification system and has been applied tomany domains. However, this approach is based on the hypothesisthat large amount of labeled data are available. To overcomethis drawback, transfer learning techniques have been proposed.These techniques use parts of large deep networks, learned byusing very large image datasets, as starting points for the learningof networks to solve a more specific classification problem. In thispaper, we present a deep transfer learning based tool to helppaleographers in identifying the parts of a manuscript that werewritten by the same writer. The proposed approach has beentested on a set of digital images from a Bible of the XII century.The achieved results confirmed the effectiveness of the proposedapproach.\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b620012b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Detecting_the_Evolution_of_Semantics = \"Individual differences in semantics and beliefs have up tonow been identified primarily by questioning people. How\u0002ever, semantics and beliefs can also be observed in concrete,quantifiable contexts such as reaction-time experiments. Herewe demonstrate an automatic mechanism which can repli\u0002cate such semantics by observing regularities in language usethrough statistical text analysis. We postulate that humanchildren, who are fantastic pattern recognizers, may also ex\u0002ploit this same information, thus our mechanism may be anessential module in a human-like cognitive system. In thisarticle we first review the underlying theories and existing re\u0002sults, then present the tool itself. We validate the tool againstexisting semantic priming reaction-time results. Finally weuse the tool to explore the evolution of beliefs extracted fromthree sources: the Bible, the works of Shakespeare and thecontemporary British National Corpus.\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e5ab1ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Graphics_based_intelligent_search = \"This paper presents an autonomous text and context-mining algorithm that converts text documents into point clouds for visual search cues. This algorithm is applied to the task of data-mining a scriptural database comprised of the Old andNew Testaments from the Bible and the Book of Mormon, Doctrine and Covenants, and the Pearl of Great Price. Results are generated which graphically show the scripture that represents the average concept of the database and the mining ofthe documents down to the verse level.\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cac93152",
   "metadata": {},
   "outputs": [],
   "source": [
    "Rolling_stylometry = \"his article introduces a new stylometric method that combines supervised ma\u0002chine-learning classification with the idea of sequential analysis. Unlike standardprocedures, aimed at assessing style differentiation between discrete text samples,the new method, supported with compact visualization, tries to look inside a textrepresented as a set of linearly sliced chunks, in order to test their stylistic con\u0002sistency. Three flavors of the method have been introduced: (1) Rolling SVM,relying on the support vector machines (SVM) classifier, (2) Rolling NSC, basedon the nearest shrunken centroids method, and (3) Rolling Delta, using theclassic Burrowsian measure of similarity. The technique is primarily intendedto assess mixed authorship; however, it can be also used as a magnifying glass toinspect works with unclear stylometric signal. To demonstrate its applicability,three different examples of collaborative work have been briefly discussed: (1) the13th-century French allegorical poem Roman de la Rose, (2) a 15th-centurytranslation of the Bible into Polish known as Queen Sophia’s Bible, and (3) TheInheritors, a novel collaboratively written by Joseph Conrad and Ford MadoxFord in 1901.\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5ab2fc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_alignment_in_early_printed = \"e describe a technique for transcript alignment in early printed books by using deep models in com\u0002bination with dynamic programming algorithms. Two object detection models, based on Faster R-CNN,are trained to locate words. We first train an initial model to recognize generic words and hyphens byusing information about the number of words in text lines. Using the model prediction on pages with aline-by-line ground-truth annotation is available, we train a second model able to detect landmark words.The alignment is then based on the identification of landmark words in pages where we only know thetext corresponding to zones in the page. The proposed technique is evaluated on a publicly available dig\u0002itization of the Gutenberg Bible while the transcription is based on the Vulgata, a late 4th century Latintranslation of the Bible.\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "678649f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Algorithmic_handwriting_analysis_of_Judah = \"The relationship between the expansion of literacy in Judah andcomposition of biblical texts has attracted scholarly attention forover a century. Information on this issue can be deduced fromHebrew inscriptions from the final phase of the first Templeperiod. We report our investigation of 16 inscriptions from theJudahite desert fortress of Arad, dated ca. 600 BCE—the eve ofNebuchadnezzar’s destruction of Jerusalem. The inquiry is basedon new methods for image processing and document analysis, aswell as machine learning algorithms. These techniques enableidentification of the minimal number of authors in a given groupof inscriptions. Our algorithmic analysis, complemented by thetextual information, reveals a minimum of six authors within theexamined inscriptions. The results indicate that in this remote fortliteracy had spread throughout the military hierarchy, down to thequartermaster and probably even below that rank. This impliesthat an educational infrastructure that could support the compo\u0002sition of literary texts in Judah already existed before the destruc\u0002tion of the first Temple. A similar level of literacy in this area isattested again only 400 y later, ca. 200 BCE.\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d6e67fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "An_end_to_end_deep_learning_system = \"This paper presents an end-to-end system to identify writers in medieval manuscripts. The proposedsystem consists in a three-step model for detection and classification of lines in the manuscript andpage writer identification. The first two steps are based on deep neural networks trained with transferlearning techniques and specialized to solve the task in hand. The third stage is a weighted majority voterow-decision combiner that assigns to each page a writer. The main goal of this paper is to study theapplicability of deep learning in this context when a relatively small training dataset is available. Wetested our system with several state-of-the-art deep architectures on a digitized manuscript known asthe Avila Bible, using only 9.6% of the total pages for training. Our approach proves to be very effectivein identifying page writers, reaching a peak of 96.48% of accuracy and 96.56% of F1 score.\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5334f5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "An_Experimental_Comparison_between_Deep = \"In the framework of palaeography, the availability of both effective image analysisalgorithms, and high-quality digital images has favored the development of new applications forthe study of ancient manuscripts and has provided new tools for decision-making support systems.The quality of the results provided by such applications, however, is strongly influenced by theselection of effective features, which should be able to capture the distinctive aspects to which thepaleography expert is interested in. This process is very difficult to generalize due to the enormousvariability in the type of ancient documents, produced in different historical periods with differentlanguages and styles. The effect is that it is very difficult to define standard techniques that aregeneral enough to be effectively used in any case, and this is the reason why ad-hoc systems,generally designed according to paleographers’ suggestions, have been designed for the analysisof ancient manuscripts. In recent years, there has been a growing scientific interest in the useof techniques based on deep learning (DL) for the automatic processing of ancient documents.This interest is not only due to their capability of designing high-performance pattern recognitionsystems, but also to their ability of automatically extracting features from raw data, without using anya priori knowledge. Moving from these considerations, the aim of this study is to verify if DL-basedapproaches may actually represent a general methodology for automatically designing machinelearning systems for palaeography applications. To this purpose, we compared the performance of aDL-based approach with that of a “classical” machine learning one, in a particularly unfavorable casefor DL, namely that of highly standardized schools. The rationale of this choice is to compare theobtainable results even when context information is present and discriminating: this information isignored by DL approaches, while it is used by machine learning methods, making the comparisonmore significant. The experimental results refer to the use of a large sets of digital images extractedfrom an entire 12th-century Bibles, the “Avila Bible”. This manuscript, produced by several scribeswho worked in different periods and in different places, represents a severe test bed to evaluate theefficiency of scribe identification systems.\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f35506ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Biblical_intertextuality_in_a_digital = \"Over centuries texts of all genres have been connected byquotes, allusions, idioms, stylistic imitations and many more.Understanding literature means understanding these kindsof intertextual relations. The goal of the G¨ottingen sub\u0002project of eTRACES,1an interdisciplinary project of hu\u0002manists and computer scientists, is to enable research onthis essential part of literary studies. We are creating a dig\u0002ital working environment, a tool called GERTRUDE (G¨ot\u0002tingen E-Research: Text Re-Use for Digital Editions), in aneffort to determine whether and to what degree such a toolcan support a researcher in finding, marking and annotatingintertextual relations:- Especially in big text corpora, looking for intertextual rela\u0002tions can be very time-consuming. So Text-Mining-algorithmsare integrated to determine so called “textual re-use”, hop\u0002ing also to find interesting textual relations not yet knownor expected (serendipity effect).- For referencing an exact text passage, the TextGrid Ci\u0002tation Schema is used, because it enables us to mark up asegment of text down to the granularity of letters and toaddress different editions of a text.- The possibilities and limitations of annotating or even eval\u0002uating a text passage in its relation to others, its form, func\u0002tion and “degree” of intertextuality will be researched by cre\u0002ating this tool as a crowd-sourcing environment: It is usableby everyone interested and it is also integrated in universitycourses, where students are encouraged to use it. By thismeans it is possible to compare and discuss the results aswell as the usability, possibilities and limitations of the tool.Our approach is based on German literature from 1500s to1900s and is part of a BMBF-sponsored text corpus availableunder a Creative Commons License online\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "65f1cb94",
   "metadata": {},
   "outputs": [],
   "source": [
    "Computational_modelling_of_an_optical_character = \"This study acquired a dataset of scanned images of Standard Yorùbá printed text andformulated a Yorùbá character image recognition model. The model formulated was im\u0002plemented and the performance of the model evaluated to develop an Optical CharacterRecognition (OCR) model for Yorùbá printed text images.The image dataset at 300 dots per inches (dpi) was acquired by generating image text\u0002line from Yorùbá New Testament Bible (Bibeli Mimo) corpus using Unicode UTF8. The LongShort Term Memory (LSTM) model, a variant of Recurrent Neural Network (RNN) was usedto formulate the Standard Yorùbá character image recognition model. The Python OCRopusframework was used to implement the model designed. The performance of the modeldesigned was evaluated using character error rate based on Levenshtein Edit Distance al\u0002gorithm.The results show that the Character Error Rate (CER) of 3.138% for the font Times NewRoman which gives better recognition than the other font style metric performance. Themodel achieved an OCR result of (7.435% CER) DejaVuSans font style image dataset, whilefor Ariel font image dataset, a result of 15.141% was achieved. The introduction of Languagemodel-based Standard Yorùbá a spell-checker corrector show a reduction in the CharacterError Rate. The Times New Roman font recorded an error rate of 1.182%, the DejaVuSansfont style at an error rate of 4.098% while the Ariel font at 5.87%.The study concluded that the performance of the model shows that the farther away animage text font is from the font(s) used in training the network, the higher the charactererror rate of the recognition and that the inclusion of a post-processing stage shows areduction in the Character Error Rates.\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b4317f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "Continuous_multilinguality_with_language = \"Most existing models for multilingual nat\u0002ural language processing (NLP) treat lan\u0002guage as a discrete category, and makepredictions for either one language or theother. In contrast, we propose usingcontinuous vector representations of lan\u0002guage. We show that these can be learnedefficiently with a character-based neurallanguage model, and used to improve in\u0002ference about language varieties not seenduring training. In experiments with 1303Bible translations into 990 different lan\u0002guages, we empirically explore the ca\u0002pacity of multilingual language models,and also show that the language vectorscapture genetic relationships between lan\u0002guages.\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5eb3dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "Design_and_Development_of_Part_of_Speech_Tagging = \"in this paper, we report on the design of a part-of-speech-tagset for Wolof and on the creation of a semi-automatically annotated goldstandard. The main motivation for this resource is to obtain data for training automatic taggers with machine learning approaches. Hence,we take machine learning considerations into account during tagset design and present training experiments as part of this paper. Thebest automatic tagger achieves an accuracy of 95.2% in cross-validation experiments. We also wanted to create a basis for experimentingwith annotation projection techniques, which exploit parallel corpora. For this reason, it was useful to use a part of the Bible as the goldstandard corpus, for which sentence-aligned parallel versions in many languages are easy to obtain.\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "36c70f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Development_of_a_Recurrent_Neural_Network_Model = \"This research developed a recurrent neural network model for English to Yoruba machine translation. Parallel corpus was obtained from the English and Yoruba bible corpus. The developed model was tested and evaluated using both manual and automatic evaluation techniques. Results from manual evaluation by ten human evaluators show that the system is adequate and fluent. Also, results from automatic evaluation shows that the developed model has decent and good translation as well as higher accuracy because it has better correlation with human judgment.\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3f94ad59",
   "metadata": {},
   "outputs": [],
   "source": [
    " Hybrid_Part_of_Speech_Tagger_for = \"The process of assigning part of speech for every word in a given sentence according to the context is called as part of speech tagging. Part of speech tagging (POS tagging) plays an important role in the area of natural language processing (NLP) including applications such as speech recognition, speech synthesis, natural language parsing, information retrieval, multi words term extraction, word sense disambiguation and machine translation. This paper proposes an efficient and accurate POS tagging technique for Malayalam language using hybrid approach. We propose a Conditional Random Fields(CRF) based method integrated with Rule-Based method. We use SVM based method to compare the accuracy. The corpus both tagged and untagged used for training and testing the system is in the unicode format. The tagset developed by lIlT Hyderabad for Indian Languages is used. The system is tested for selected books of Bible and perform with an accuracy of94%\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bbca9336",
   "metadata": {},
   "outputs": [],
   "source": [
    "If_all_you_have_is_a_bit_of_the_Bible = \"We present a simple method for learning part\u0002of-speech taggers for languages like Akawaio,Aukan, or Cakchiquel – languages for whichnothing but a translation of parts of the Bibleexists. By aggregating over the tags from afew annotated languages and spreading themvia word-alignment on the verses, we learnPOS taggers for 100 languages, using the lan\u0002guages to bootstrap each other. We evalu\u0002ate our cross-lingual models on the 25 lan\u0002guages where test sets exist. Our approachperforms much better (20-30%) than state\u0002of-the-art unsupervised POS taggers inducedfrom Bible translations, and is often compet\u0002itive with weakly supervised approaches thatassume high-quality parallel corpora, repre\u0002sentative monolingual corpora with perfect to\u0002kenization, and/or tag dictionaries. We makemodels for all 100\".lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c59dc06",
   "metadata": {},
   "outputs": [],
   "source": [
    "If_You_Even_Dont_Have_a_Bit_of_Bible_Learning_Delexicalized = \"art-of-speech (POS) induction is one of the most popular tasks in research on unsupervised NLP. Various unsupervised and semi\u0002supervised methods have been proposed to tag an unseen language. However, many of them require some partial understanding of thetarget language because they rely on dictionaries or parallel corpora such as the Bible. In this paper, we propose a different methodnamed delexicalized tagging, for which we only need a raw corpus of the target language. We transfer tagging models trained onannotated corpora of one or more resource-rich languages. We employ language-independent features such as word length, frequency,neighborhood entropy, character classes (alphabetic vs. numeric vs. punctuation) etc. We demonstrate that such features can, to certainextent, serve as predictors of the part of speech, represented by the universal POS tag (Das and Petrov, 2011).Keywords: delexicalized tagging, HamdleDT 2.0, features expansion, classifier\".lower() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b5784a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "LVQ_for_text_categorization_using_a_multilingual = \"Neural learning has been used with e6ectiveness in natural language processing tasks. Par\u0002ticularly, the Widrow–Ho6 and the Kivinen–Warmuth exponentiated gradient (based on neurallearning rules) algorithms have been used in text categorization, improving the results obtainedby the well-known Rocchio’s algorithm. The high performance of competitive learning algo\u0002rithms, recently applied to solve information retrieval problems, leads us to use them in thespeci=c text categorization tasks. This paper presents a multilingual categorization system basedon neural learning, using the polyglot Bible as training collection, both in Spanish and English.The method we suggest is based on using the LVQalgorithm to build a classi=er that learns thetraining multilingual collection. We have performed experiments with the four algorithm whichshow that the ideas we describe are promising and are worth further investigation\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15d9ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Modeling_the_Hebrew_Bible_Potential = \"Providing useful and efficient semantic annotations is a major challengefor knowledge design of any body of text, especially historical documents. Inthis article, we propose Topic Modeling as an important first step to gathersemantic information beyond the lexicon which can be added as annotationsin the SHEBANQ. By laying out a case study, we discuss both noise andstructure found in comparing topics extracted within different distributions,and show the value of such approach, which we label a topic hierarchy. Wealso show a first result in applying such approach to study diachronic varietyin the Bible, and show how this overall Topic Modeling approach can resultin more query options for users of the database\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b86f0e31",
   "metadata": {},
   "outputs": [],
   "source": [
    "Normalizing_Historical_Orthography = \"Historical text presents numerous challenges for contempo\u0002rary different techniques, e.g. information retrieval, OCRand POS tagging. In particular, the absence of consistentorthographic conventions in historical text presents difficul\u0002ties for any system which requires reference to a fixed lexiconaccessed by orthographic form. For example, language mod\u0002eling or retrieval engine for historical text which is producedby OCR systems, where the spelling of words often differ invarious way, e.g. one word might have different spellingsevolved over time. It is very important to aid those tech\u0002niques with the rules for automatic mapping of historicalwordforms. In this paper, we propose a new technique tomodel the target modern language by means of a recurrentneural network with long-short term memory architecture.Because the network is recurrent, the considered context isnot limited to a fixed size especially due to memory cellswhich are designed to deal with long-term dependencies.In the set of experiments conducted on the Luther bibledatabase and transform wordforms from Early New HighGerman (ENHG) 14th - 16th centuries to the correspond\u0002ing modern wordforms in New High German (NHG). Wecompare our proposed supervised model LSTM to variousmethods for computing word alignments using statistical,heuristic models. Our new proposed LSTM outperformsthe other three state-of-the-art methods. The evaluationshows the accuracy of our model on the known wordforms is93.90% and on the unknown wordforms is 87.95%, while theaccuracy of the existing state-of-the-art combined approachof the wordlist-based and rule-based normalization models is92.93% for known and 76.88% for unknown tokens. Our pro\u0002posed LSTM model outperforms on normalizing the modernwordform to historical wordform. The performance on seentokens is 93.4%, while for unknown tokens is 89.17\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9ffddf74",
   "metadata": {},
   "outputs": [],
   "source": [
    "Ontology_Based_CBR_System_on_Religious = \"The Bible, which is the holy book for Christians, contains instructions for life and has a vastreservoir of information, useful for life. It is sufficient to provide an answer to queries onany vital topic in life. The paper focuses on ‘Leadership’, which is an important concept inlife and aims at developing Ontology for the same, based on a few books of the Old and NewTestaments of the Bible. Leadership is one of the vital personality traits that decide howmuch a person achieves and climbs up the ladder of success in life and influences otherpeople and the society at large. The study on leadership was done from various perspectives—historical, sociological, psychological, etc. However, in this work, it is approached from aBiblical perspective, wherein the Biblical truths are used both to diagnose the leadershippotential in a person and recommend leadership scope for a person based on his or herleadership abilities. A combination of ontological approach and Case-Based Reasoning(CBR) is used for this. The Ontology Web Language (OWL) on leadership is developed andused in the CBR system development. The CBR system is developed using myCBR. Any usercan use this application to assess his leadership traits, recognize his eligibility for becominga leader and receive biblical recommendations to equip him for future leadership.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "e18567b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "Prototype_Matching_Finding_Meaning = \"it is common that text documents are characterised andclassified by keywords and that the authors use to give andname these text characteristics. Visa et al. have, however,developed a new methodology based on prototype matching.The prototype is an interesting document or a part of anextracted, interesting text. This prototype is matched withthe existing document database or the monitored documentflow. Our claim is that the new methodology is capable ofextracting meaning automatically from the contents of thedocument. To verify this hypothesis a test was designedwith the Bible. Two different translations, one in Englishand another in Finnish, were selected as test text material.Verification tests that included the search of the ten nearestbooks to every book of the Bible were performed with a de\u0002signed prototype version of the software application. Theinteresting test results are reported in this paper.The new methodology is based on a hierarchy of Self\u0002Organizing Maps (SOM) and on a smart encoding of words.The words of a text document are encoded. The encodedwords are represented as word vectors. The word vectorsare clustered by the SOM and this process creates a wordmap. The words of a text document are replaced with theaddresses on the word map. Now the document consists ofa sequence of addresses. These addresses contain informa\u0002tion of word order. The document is considered sentenceby sentence. These sentence vectors are clustered by SOM.This process creates a sentence map. Now the sentences ofthe text document are replaced with addresses on the sen\u0002\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "de22ed1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Reliable_writer_identification_in_medieval_manuscripts = \"In the field of manuscript studies (palaeography and codicology), a particularly interesting case is the study ofhighly standardized handwriting and book typologies. In such cases, the analysis of some basic layout features,mainly related to the organization of the page and to the exploitation of the available space, may be very helpfulfor distinguishing similar scribal hands. In this framework, we have defined a set of layout features to develop apattern recognition system for identifying the scribes who collaborated to the transcription of a single medievalLatin book. We have also experimentally characterized the discriminative power of each considered feature andwe have verified whether the selection of an appropriate subset of features for each scribe, specifically devisedfor distinguishing him from all the others, could allow us to achieve better results. This approach allowed usto introduce in a very simple way a reject option for rejecting unreliably classified samples, namely thosenot assigned to any scribe or assigned to more scribes. The experiments, performed on a large database ofdigital images from the so called ‘‘Avila Bible’’ – a giant Latin copy of the whole Bible produced during the XIIcentury between Italy and Spain – confirmed the effectiveness of the proposed method. Finally, we made publiclyavailable the data set extracted from the Avila Bible images\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "d2f00e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "Semantic_Vector_Combinations_and_the_Synoptic = \"This paper applies some recent methods involving semanticvectors and their combination operations to some very traditional ques\u0002tions, including the discovery of similarities and differences between thefour Gospels, relationships between individuals, and the identification ofgeopolitical regions and leaders in the ancient world. In the process, weemploy several methods from linear algebra and vector space models,some of which are of particular importance in quantum mechanics andquantum logic.Our conclusions are in general positive: the vector methods do a good jobof capturing well-known facts about the Bible, its authors, and relation\u0002ships between people and places mentioned in the Bible. On the morespecific topic of quantum as opposed to other approaches, our conclusionsare more mixed: on the whole, we do not find evidence for preferring vec\u0002tor methods that are directly associated with quantum mechanics overvector methods developed independently of quantum mechanics. We sug\u0002gest that this argues for synthesis rather than division between classicaland quantum models for information processing\".lower()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "05d7ce77",
   "metadata": {},
   "outputs": [],
   "source": [
    "Stylometric_classification_of_different_translations = \"Quantitative stylometry of ten translations of the same Bible passage into English,followed by Ward clustering, produces a dendrogram that reflects the well-knownhistory and intent of the translations. We conclude that quantitative stylometrycombined with clustering is a useful tool for reconstructing literary history\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cf0ff8b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_Correction_Using_Approaches_Based_on_Markovian = \"everal authors have reported interesting results ob\u0002tained by using untrained randomly initialized recurrentpart of an recurrent neural network (RNN). Instead of long,difficult and often unnecessary adaptation process, dynam\u0002ics based on fixed point attractors can be rich enough forfurther exploitation for some tasks. The principle explain\u0002ing untrained RNN state space structure is called Marko\u0002vian architectural bias [1, 8] and several methods usingthis behavior were studied. In this paper we apply theseapproaches to correct corrupted symbols from symbol se\u0002quence. These approaches share some properties with vari\u0002able length Markov models hence our experiments are in\u0002spired by the paper dealing with the text correction on thebible dataset\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "b44dd863",
   "metadata": {},
   "outputs": [],
   "source": [
    "Text_Mining_Analysis_of_the_King_James = \"English as a Second Language (ESL) students are oftenexpected to read and use English translations of the Bible foracademic and worship purposes. The purpose of this studywas to explore the writing style of the King James Versionand the New International Version in terms of each transla\u0002tion’s formality, readability, and sentiment using quantitativetext mining analysis tools. Results indicated that the KJV usedmore formal language, had a higher grade level of readabilityand used slightly more positive wording than the NIV. In add\u0002ition, for both translations the Old Testament was much morenegative in terms of sentiment than the New Testament.Lastly, a moderate positive relationship was found betweenreadability and formality for both translations.\".lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "e4577530",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nltk.word_tokenize(Text_Mining_Analysis_of_the_King_James)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "91db00dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "fc6918b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentence1 = [w for w in tokens if not w in stop_words] \n",
    "  \n",
    "filtered_sentence1 = []\n",
    "  \n",
    "for w in tokens: \n",
    "    if w not in stop_words: \n",
    "        filtered_sentence1.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "953ee881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['english', 'second', 'language', '(', 'esl', ')', 'students', 'oftenexpected', 'read', 'use', 'english', 'translations', 'bible', 'foracademic', 'worship', 'purposes', '.', 'purpose', 'studywas', 'explore', 'writing', 'style', 'king', 'james', 'versionand', 'new', 'international', 'version', 'terms', 'transla\\x02tion', '’', 'formality', ',', 'readability', ',', 'sentiment', 'using', 'quantitativetext', 'mining', 'analysis', 'tools', '.', 'results', 'indicated', 'kjv', 'usedmore', 'formal', 'language', ',', 'higher', 'grade', 'level', 'readabilityand', 'used', 'slightly', 'positive', 'wording', 'niv', '.', 'add\\x02ition', ',', 'translations', 'old', 'testament', 'much', 'morenegative', 'terms', 'sentiment', 'new', 'testament.lastly', ',', 'moderate', 'positive', 'relationship', 'found', 'betweenreadability', 'formality', 'translations', '.']\n"
     ]
    }
   ],
   "source": [
    "print(filtered_sentence1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "b650e4c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =np.array([[\"CONTEXT_BASED_MACHINE_TRANSLATION\", \"'current', 'approaches', 'machine', 'translation', 'usually', 'require', 'large', 'set', 'parallel', 'corpus', 'order', 'achieve', 'fluency', 'like', 'case', 'neural', 'machine', 'translation', 'nmt', ',', 'statistical', 'machine', 'translation', 'smt', 'example-based', 'machinetranslation', 'ebmt', '.', 'context', 'awareness', 'phrase-based', 'machine', 'translation', 'pbmt', 'approaches', 'also', 'questionable', '.', 'research', 'develops', 'system', 'thattranslates', 'english', 'text', 'amharic', 'text', 'using', 'combination', 'context', 'based', 'machine', 'translation', 'cbmt', 'recurrent', 'neural', 'network', 'machine', 'translation', 'rnnmt', '.', 'built', 'bilingual', 'dictionary', 'cbmt', 'system', 'use', 'along', 'witha', 'large', 'target', 'corpus', '.', 'rnnmt', 'model', 'provided', 'output', 'cbmt', 'parallel', 'corpus', 'training', '.', 'combinational', 'approachon', 'english-amharic', 'language', 'pair', 'yields', 'performance', 'improvement', 'thesimple', 'neural', 'machine', 'translation', 'nmt', '.'\"],\n",
    "               [\"Deep_Transfer_Learning_for_writer\", \"'digital', 'paleography', ',', 'recent', 'technology', 'advance\\x02ments', 'used', 'support', 'paleographers', 'study', 'andanalysis', 'ancient', 'documents', '.', 'one', 'main', 'goal', 'paleographersis', 'identify', 'different', 'scribes', '(', 'writers', ')', 'wrote', 'givenmanuscript', '.', 'deep', 'learning', 'recently', 'received', 'much', 'attentionfrom', 'researchers', 'classification', 'system', 'applied', 'tomany', 'domains', '.', 'however', ',', 'approach', 'based', 'hypothesisthat', 'large', 'amount', 'labeled', 'data', 'available', '.', 'overcomethis', 'drawback', ',', 'transfer', 'learning', 'techniques', 'proposed.these', 'techniques', 'use', 'parts', 'large', 'deep', 'networks', ',', 'learned', 'byusing', 'large', 'image', 'datasets', ',', 'starting', 'points', 'learningof', 'networks', 'solve', 'specific', 'classification', 'problem', '.', 'thispaper', ',', 'present', 'deep', 'transfer', 'learning', 'based', 'tool', 'helppaleographers', 'identifying', 'parts', 'manuscript', 'werewritten', 'writer', '.', 'proposed', 'approach', 'beentested', 'set', 'digital', 'images', 'bible', 'xii', 'century.the', 'achieved', 'results', 'confirmed', 'effectiveness', 'proposedapproach', '.'\"],\n",
    "               [\"Detecting_the_Evolution_of_Semantics\", \"'individual', 'differences', 'semantics', 'beliefs', 'tonow', 'identified', 'primarily', 'questioning', 'people', '.', 'how\\x02ever', ',', 'semantics', 'beliefs', 'also', 'observed', 'concrete', ',', 'quantifiable', 'contexts', 'reaction-time', 'experiments', '.', 'herewe', 'demonstrate', 'automatic', 'mechanism', 'repli\\x02cate', 'semantics', 'observing', 'regularities', 'language', 'usethrough', 'statistical', 'text', 'analysis', '.', 'postulate', 'humanchildren', ',', 'fantastic', 'pattern', 'recognizers', ',', 'may', 'also', 'ex\\x02ploit', 'information', ',', 'thus', 'mechanism', 'may', 'anessential', 'module', 'human-like', 'cognitive', 'system', '.', 'thisarticle', 'first', 'review', 'underlying', 'theories', 'existing', 're\\x02sults', ',', 'present', 'tool', '.', 'validate', 'tool', 'againstexisting', 'semantic', 'priming', 'reaction-time', 'results', '.', 'finally', 'weuse', 'tool', 'explore', 'evolution', 'beliefs', 'extracted', 'fromthree', 'sources', ':', 'bible', ',', 'works', 'shakespeare', 'thecontemporary', 'british', 'national', 'corpus', '.'\"],\n",
    "               [\"Graphics_based_intelligent_search\", \"'paper', 'presents', 'autonomous', 'text', 'context-mining', 'algorithm', 'converts', 'text', 'documents', 'point', 'clouds', 'visual', 'search', 'cues', '.', 'algorithm', 'applied', 'task', 'data-mining', 'scriptural', 'database', 'comprised', 'old', 'andnew', 'testaments', 'bible', 'book', 'mormon', ',', 'doctrine', 'covenants', ',', 'pearl', 'great', 'price', '.', 'results', 'generated', 'graphically', 'show', 'scripture', 'represents', 'average', 'concept', 'database', 'mining', 'ofthe', 'documents', 'verse', 'level', '.'\"],\n",
    "               [\"Rolling_stylometry\", \"'article', 'introduces', 'new', 'stylometric', 'method', 'combines', 'supervised', 'ma\\x02chine-learning', 'classification', 'idea', 'sequential', 'analysis', '.', 'unlike', 'standardprocedures', ',', 'aimed', 'assessing', 'style', 'differentiation', 'discrete', 'text', 'samples', ',', 'new', 'method', ',', 'supported', 'compact', 'visualization', ',', 'tries', 'look', 'inside', 'textrepresented', 'set', 'linearly', 'sliced', 'chunks', ',', 'order', 'test', 'stylistic', 'con\\x02sistency', '.', 'three', 'flavors', 'method', 'introduced', ':', '(', '1', ')', 'rolling', 'svm', ',', 'relying', 'support', 'vector', 'machines', '(', 'svm', ')', 'classifier', ',', '(', '2', ')', 'rolling', 'nsc', ',', 'basedon', 'nearest', 'shrunken', 'centroids', 'method', ',', '(', '3', ')', 'rolling', 'delta', ',', 'using', 'theclassic', 'burrowsian', 'measure', 'similarity', '.', 'technique', 'primarily', 'intendedto', 'assess', 'mixed', 'authorship', ';', 'however', ',', 'also', 'used', 'magnifying', 'glass', 'toinspect', 'works', 'unclear', 'stylometric', 'signal', '.', 'demonstrate', 'applicability', ',', 'three', 'different', 'examples', 'collaborative', 'work', 'briefly', 'discussed', ':', '(', '1', ')', 'the13th-century', 'french', 'allegorical', 'poem', 'roman', 'de', 'la', 'rose', ',', '(', '2', ')', '15th-centurytranslation', 'bible', 'polish', 'known', 'queen', 'sophia', '’', 'bible', ',', '(', '3', ')', 'theinheritors', ',', 'novel', 'collaboratively', 'written', 'joseph', 'conrad', 'ford', 'madoxford', '1901', '.'\"],\n",
    "               [\"Text_alignment_in_early_printed\", \"'e', 'describe', 'technique', 'transcript', 'alignment', 'early', 'printed', 'books', 'using', 'deep', 'models', 'com\\x02bination', 'dynamic', 'programming', 'algorithms', '.', 'two', 'object', 'detection', 'models', ',', 'based', 'faster', 'r-cnn', ',', 'trained', 'locate', 'words', '.', 'first', 'train', 'initial', 'model', 'recognize', 'generic', 'words', 'hyphens', 'byusing', 'information', 'number', 'words', 'text', 'lines', '.', 'using', 'model', 'prediction', 'pages', 'aline-by-line', 'ground-truth', 'annotation', 'available', ',', 'train', 'second', 'model', 'able', 'detect', 'landmark', 'words.the', 'alignment', 'based', 'identification', 'landmark', 'words', 'pages', 'know', 'thetext', 'corresponding', 'zones', 'page', '.', 'proposed', 'technique', 'evaluated', 'publicly', 'available', 'dig\\x02itization', 'gutenberg', 'bible', 'transcription', 'based', 'vulgata', ',', 'late', '4th', 'century', 'latintranslation', 'bible', '.'\"],\n",
    "               [\"Algorithmic_handwriting_analysis_of_Judah\", \"'relationship', 'expansion', 'literacy', 'judah', 'andcomposition', 'biblical', 'texts', 'attracted', 'scholarly', 'attention', 'forover', 'century', '.', 'information', 'issue', 'deduced', 'fromhebrew', 'inscriptions', 'final', 'phase', 'first', 'templeperiod', '.', 'report', 'investigation', '16', 'inscriptions', 'thejudahite', 'desert', 'fortress', 'arad', ',', 'dated', 'ca', '.', '600', 'bce—the', 'eve', 'ofnebuchadnezzar', '’', 'destruction', 'jerusalem', '.', 'inquiry', 'basedon', 'new', 'methods', 'image', 'processing', 'document', 'analysis', ',', 'aswell', 'machine', 'learning', 'algorithms', '.', 'techniques', 'enableidentification', 'minimal', 'number', 'authors', 'given', 'groupof', 'inscriptions', '.', 'algorithmic', 'analysis', ',', 'complemented', 'thetextual', 'information', ',', 'reveals', 'minimum', 'six', 'authors', 'within', 'theexamined', 'inscriptions', '.', 'results', 'indicate', 'remote', 'fortliteracy', 'spread', 'throughout', 'military', 'hierarchy', ',', 'thequartermaster', 'probably', 'even', 'rank', '.', 'impliesthat', 'educational', 'infrastructure', 'could', 'support', 'compo\\x02sition', 'literary', 'texts', 'judah', 'already', 'existed', 'destruc\\x02tion', 'first', 'temple', '.', 'similar', 'level', 'literacy', 'area', 'isattested', '400', 'later', ',', 'ca', '.', '200', 'bce', '.'\"],\n",
    "               [\"An_end_to_end_deep_learning_system\", \"'paper', 'presents', 'end-to-end', 'system', 'identify', 'writers', 'medieval', 'manuscripts', '.', 'proposedsystem', 'consists', 'three-step', 'model', 'detection', 'classification', 'lines', 'manuscript', 'andpage', 'writer', 'identification', '.', 'first', 'two', 'steps', 'based', 'deep', 'neural', 'networks', 'trained', 'transferlearning', 'techniques', 'specialized', 'solve', 'task', 'hand', '.', 'third', 'stage', 'weighted', 'majority', 'voterow-decision', 'combiner', 'assigns', 'page', 'writer', '.', 'main', 'goal', 'paper', 'study', 'theapplicability', 'deep', 'learning', 'context', 'relatively', 'small', 'training', 'dataset', 'available', '.', 'wetested', 'system', 'several', 'state-of-the-art', 'deep', 'architectures', 'digitized', 'manuscript', 'known', 'asthe', 'avila', 'bible', ',', 'using', '9.6', '%', 'total', 'pages', 'training', '.', 'approach', 'proves', 'effectivein', 'identifying', 'page', 'writers', ',', 'reaching', 'peak', '96.48', '%', 'accuracy', '96.56', '%', 'f1', 'score', '.'\"],\n",
    "               [\"An_Experimental_Comparison_between_Deep\", \"'framework', 'palaeography', ',', 'availability', 'effective', 'image', 'analysisalgorithms', ',', 'high-quality', 'digital', 'images', 'favored', 'development', 'new', 'applications', 'forthe', 'study', 'ancient', 'manuscripts', 'provided', 'new', 'tools', 'decision-making', 'support', 'systems.the', 'quality', 'results', 'provided', 'applications', ',', 'however', ',', 'strongly', 'influenced', 'theselection', 'effective', 'features', ',', 'able', 'capture', 'distinctive', 'aspects', 'thepaleography', 'expert', 'interested', '.', 'process', 'difficult', 'generalize', 'due', 'enormousvariability', 'type', 'ancient', 'documents', ',', 'produced', 'different', 'historical', 'periods', 'differentlanguages', 'styles', '.', 'effect', 'difficult', 'define', 'standard', 'techniques', 'aregeneral', 'enough', 'effectively', 'used', 'case', ',', 'reason', 'ad-hoc', 'systems', ',', 'generally', 'designed', 'according', 'paleographers', '’', 'suggestions', ',', 'designed', 'analysisof', 'ancient', 'manuscripts', '.', 'recent', 'years', ',', 'growing', 'scientific', 'interest', 'useof', 'techniques', 'based', 'deep', 'learning', '(', 'dl', ')', 'automatic', 'processing', 'ancient', 'documents.this', 'interest', 'due', 'capability', 'designing', 'high-performance', 'pattern', 'recognitionsystems', ',', 'also', 'ability', 'automatically', 'extracting', 'features', 'raw', 'data', ',', 'without', 'using', 'anya', 'priori', 'knowledge', '.', 'moving', 'considerations', ',', 'aim', 'study', 'verify', 'dl-basedapproaches', 'may', 'actually', 'represent', 'general', 'methodology', 'automatically', 'designing', 'machinelearning', 'systems', 'palaeography', 'applications', '.', 'purpose', ',', 'compared', 'performance', 'adl-based', 'approach', '“', 'classical', '”', 'machine', 'learning', 'one', ',', 'particularly', 'unfavorable', 'casefor', 'dl', ',', 'namely', 'highly', 'standardized', 'schools', '.', 'rationale', 'choice', 'compare', 'theobtainable', 'results', 'even', 'context', 'information', 'present', 'discriminating', ':', 'information', 'isignored', 'dl', 'approaches', ',', 'used', 'machine', 'learning', 'methods', ',', 'making', 'comparisonmore', 'significant', '.', 'experimental', 'results', 'refer', 'use', 'large', 'sets', 'digital', 'images', 'extractedfrom', 'entire', '12th-century', 'bibles', ',', '“', 'avila', 'bible', '”', '.', 'manuscript', ',', 'produced', 'several', 'scribeswho', 'worked', 'different', 'periods', 'different', 'places', ',', 'represents', 'severe', 'test', 'bed', 'evaluate', 'theefficiency', 'scribe', 'identification', 'systems', '.'\"],\n",
    "               [\"Biblical_intertextuality_in_a_digital\", \"'centuries', 'texts', 'genres', 'connected', 'byquotes', ',', 'allusions', ',', 'idioms', ',', 'stylistic', 'imitations', 'many', 'more.understanding', 'literature', 'means', 'understanding', 'kindsof', 'intertextual', 'relations', '.', 'goal', 'g¨ottingen', 'sub\\x02project', 'etraces,1an', 'interdisciplinary', 'project', 'hu\\x02manists', 'computer', 'scientists', ',', 'enable', 'research', 'onthis', 'essential', 'part', 'literary', 'studies', '.', 'creating', 'dig\\x02ital', 'working', 'environment', ',', 'tool', 'called', 'gertrude', '(', 'g¨ot\\x02tingen', 'e-research', ':', 'text', 're-use', 'digital', 'editions', ')', ',', 'aneffort', 'determine', 'whether', 'degree', 'toolcan', 'support', 'researcher', 'finding', ',', 'marking', 'annotatingintertextual', 'relations', ':', '-', 'especially', 'big', 'text', 'corpora', ',', 'looking', 'intertextual', 'rela\\x02tions', 'time-consuming', '.', 'text-mining-algorithmsare', 'integrated', 'determine', 'called', '“', 'textual', 're-use', '”', ',', 'hop\\x02ing', 'also', 'find', 'interesting', 'textual', 'relations', 'yet', 'knownor', 'expected', '(', 'serendipity', 'effect', ')', '.-', 'referencing', 'exact', 'text', 'passage', ',', 'textgrid', 'ci\\x02tation', 'schema', 'used', ',', 'enables', 'us', 'mark', 'asegment', 'text', 'granularity', 'letters', 'toaddress', 'different', 'editions', 'text.-', 'possibilities', 'limitations', 'annotating', 'even', 'eval\\x02uating', 'text', 'passage', 'relation', 'others', ',', 'form', ',', 'func\\x02tion', '“', 'degree', '”', 'intertextuality', 'researched', 'cre\\x02ating', 'tool', 'crowd-sourcing', 'environment', ':', 'usableby', 'everyone', 'interested', 'also', 'integrated', 'universitycourses', ',', 'students', 'encouraged', 'use', '.', 'thismeans', 'possible', 'compare', 'discuss', 'results', 'aswell', 'usability', ',', 'possibilities', 'limitations', 'tool.our', 'approach', 'based', 'german', 'literature', '1500s', 'to1900s', 'part', 'bmbf-sponsored', 'text', 'corpus', 'availableunder', 'creative', 'commons', 'license', 'online'\"],\n",
    "               [\"Computational_modelling_of_an_optical_character\", \"'study', 'acquired', 'dataset', 'scanned', 'images', 'standard', 'yorùbá', 'printed', 'text', 'andformulated', 'yorùbá', 'character', 'image', 'recognition', 'model', '.', 'model', 'formulated', 'im\\x02plemented', 'performance', 'model', 'evaluated', 'develop', 'optical', 'characterrecognition', '(', 'ocr', ')', 'model', 'yorùbá', 'printed', 'text', 'images.the', 'image', 'dataset', '300', 'dots', 'per', 'inches', '(', 'dpi', ')', 'acquired', 'generating', 'image', 'text\\x02line', 'yorùbá', 'new', 'testament', 'bible', '(', 'bibeli', 'mimo', ')', 'corpus', 'using', 'unicode', 'utf8', '.', 'longshort', 'term', 'memory', '(', 'lstm', ')', 'model', ',', 'variant', 'recurrent', 'neural', 'network', '(', 'rnn', ')', 'usedto', 'formulate', 'standard', 'yorùbá', 'character', 'image', 'recognition', 'model', '.', 'python', 'ocropusframework', 'used', 'implement', 'model', 'designed', '.', 'performance', 'modeldesigned', 'evaluated', 'using', 'character', 'error', 'rate', 'based', 'levenshtein', 'edit', 'distance', 'al\\x02gorithm.the', 'results', 'show', 'character', 'error', 'rate', '(', 'cer', ')', '3.138', '%', 'font', 'times', 'newroman', 'gives', 'better', 'recognition', 'font', 'style', 'metric', 'performance', '.', 'themodel', 'achieved', 'ocr', 'result', '(', '7.435', '%', 'cer', ')', 'dejavusans', 'font', 'style', 'image', 'dataset', ',', 'whilefor', 'ariel', 'font', 'image', 'dataset', ',', 'result', '15.141', '%', 'achieved', '.', 'introduction', 'languagemodel-based', 'standard', 'yorùbá', 'spell-checker', 'corrector', 'show', 'reduction', 'charactererror', 'rate', '.', 'times', 'new', 'roman', 'font', 'recorded', 'error', 'rate', '1.182', '%', ',', 'dejavusansfont', 'style', 'error', 'rate', '4.098', '%', 'ariel', 'font', '5.87', '%', '.the', 'study', 'concluded', 'performance', 'model', 'shows', 'farther', 'away', 'animage', 'text', 'font', 'font', '(', ')', 'used', 'training', 'network', ',', 'higher', 'charactererror', 'rate', 'recognition', 'inclusion', 'post-processing', 'stage', 'shows', 'areduction', 'character', 'error', 'rates', '.'\"],\n",
    "               [\"Continuous_multilinguality_with_language\", \"'existing', 'models', 'multilingual', 'nat\\x02ural', 'language', 'processing', '(', 'nlp', ')', 'treat', 'lan\\x02guage', 'discrete', 'category', ',', 'makepredictions', 'either', 'one', 'language', 'theother', '.', 'contrast', ',', 'propose', 'usingcontinuous', 'vector', 'representations', 'lan\\x02guage', '.', 'show', 'learnedefficiently', 'character-based', 'neurallanguage', 'model', ',', 'used', 'improve', 'in\\x02ference', 'language', 'varieties', 'seenduring', 'training', '.', 'experiments', '1303bible', 'translations', '990', 'different', 'lan\\x02guages', ',', 'empirically', 'explore', 'ca\\x02pacity', 'multilingual', 'language', 'models', ',', 'also', 'show', 'language', 'vectorscapture', 'genetic', 'relationships', 'lan\\x02guages', '.'\"],\n",
    "               [\"Design_and_Development_of_Part_of_Speech_Tagging\", \"'paper', ',', 'report', 'design', 'part-of-speech-tagset', 'wolof', 'creation', 'semi-automatically', 'annotated', 'goldstandard', '.', 'main', 'motivation', 'resource', 'obtain', 'data', 'training', 'automatic', 'taggers', 'machine', 'learning', 'approaches', '.', 'hence', ',', 'take', 'machine', 'learning', 'considerations', 'account', 'tagset', 'design', 'present', 'training', 'experiments', 'part', 'paper', '.', 'thebest', 'automatic', 'tagger', 'achieves', 'accuracy', '95.2', '%', 'cross-validation', 'experiments', '.', 'also', 'wanted', 'create', 'basis', 'experimentingwith', 'annotation', 'projection', 'techniques', ',', 'exploit', 'parallel', 'corpora', '.', 'reason', ',', 'useful', 'use', 'part', 'bible', 'goldstandard', 'corpus', ',', 'sentence-aligned', 'parallel', 'versions', 'many', 'languages', 'easy', 'obtain', '.'\"], \n",
    "               [\"Development_of_a_Recurrent_Neural_Network_Model\", \"'research', 'developed', 'recurrent', 'neural', 'network', 'model', 'english', 'yoruba', 'machine', 'translation', '.', 'parallel', 'corpus', 'obtained', 'english', 'yoruba', 'bible', 'corpus', '.', 'developed', 'model', 'tested', 'evaluated', 'using', 'manual', 'automatic', 'evaluation', 'techniques', '.', 'results', 'manual', 'evaluation', 'ten', 'human', 'evaluators', 'show', 'system', 'adequate', 'fluent', '.', 'also', ',', 'results', 'automatic', 'evaluation', 'shows', 'developed', 'model', 'decent', 'good', 'translation', 'well', 'higher', 'accuracy', 'better', 'correlation', 'human', 'judgment', '.'\"],\n",
    "               [\"Hybrid_Part_of_Speech_Tagger_for\", \"'The', 'process', 'assigning', 'part', 'speech', 'every', 'word', 'given', 'sentence', 'according', 'context', 'called', 'part', 'speech', 'tagging', '.', 'Part', 'speech', 'tagging', '(', 'POS', 'tagging', ')', 'plays', 'important', 'role', 'area', 'natural', 'language', 'processing', '(', 'NLP', ')', 'including', 'applications', 'speech', 'recognition', ',', 'speech', 'synthesis', ',', 'natural', 'language', 'parsing', ',', 'information', 'retrieval', ',', 'multi', 'words', 'term', 'extraction', ',', 'word', 'sense', 'disambiguation', 'machine', 'translation', '.', 'This', 'paper', 'proposes', 'efficient', 'accurate', 'POS', 'tagging', 'technique', 'Malayalam', 'language', 'using', 'hybrid', 'approach', '.', 'We', 'propose', 'Conditional', 'Random', 'Fields', '(', 'CRF', ')', 'based', 'method', 'integrated', 'Rule-Based', 'method', '.', 'We', 'use', 'SVM', 'based', 'method', 'compare', 'accuracy', '.', 'The', 'corpus', 'tagged', 'untagged', 'used', 'training', 'testing', 'system', 'unicode', 'format', '.', 'The', 'tagset', 'developed', 'lIlT', 'Hyderabad', 'Indian', 'Languages', 'used', '.', 'The', 'system', 'tested', 'selected', 'books', 'Bible', 'perform', 'accuracy', 'of94', '%'\"],\n",
    "               [\"If_all_you_have_is_a_bit_of_the_Bible\", \"'present', 'simple', 'method', 'learning', 'part\\x02of-speech', 'taggers', 'languages', 'like', 'akawaio', ',', 'aukan', ',', 'cakchiquel', '–', 'languages', 'whichnothing', 'translation', 'parts', 'bibleexists', '.', 'aggregating', 'tags', 'afew', 'annotated', 'languages', 'spreading', 'themvia', 'word-alignment', 'verses', ',', 'learnpos', 'taggers', '100', 'languages', ',', 'using', 'lan\\x02guages', 'bootstrap', '.', 'evalu\\x02ate', 'cross-lingual', 'models', '25', 'lan\\x02guages', 'test', 'sets', 'exist', '.', 'approachperforms', 'much', 'better', '(', '20-30', '%', ')', 'state\\x02of-the-art', 'unsupervised', 'pos', 'taggers', 'inducedfrom', 'bible', 'translations', ',', 'often', 'compet\\x02itive', 'weakly', 'supervised', 'approaches', 'thatassume', 'high-quality', 'parallel', 'corpora', ',', 'repre\\x02sentative', 'monolingual', 'corpora', 'perfect', 'to\\x02kenization', ',', 'and/or', 'tag', 'dictionaries', '.', 'makemodels', '100'\"],\n",
    "               [\"If_You_Even_Dont_Have_a_Bit_of_Bible_Learning_Delexicalized\", \"'art-of-speech', '(', 'pos', ')', 'induction', 'one', 'popular', 'tasks', 'research', 'unsupervised', 'nlp', '.', 'various', 'unsupervised', 'semi\\x02supervised', 'methods', 'proposed', 'tag', 'unseen', 'language', '.', 'however', ',', 'many', 'require', 'partial', 'understanding', 'thetarget', 'language', 'rely', 'dictionaries', 'parallel', 'corpora', 'bible', '.', 'paper', ',', 'propose', 'different', 'methodnamed', 'delexicalized', 'tagging', ',', 'need', 'raw', 'corpus', 'target', 'language', '.', 'transfer', 'tagging', 'models', 'trained', 'onannotated', 'corpora', 'one', 'resource-rich', 'languages', '.', 'employ', 'language-independent', 'features', 'word', 'length', ',', 'frequency', ',', 'neighborhood', 'entropy', ',', 'character', 'classes', '(', 'alphabetic', 'vs.', 'numeric', 'vs.', 'punctuation', ')', 'etc', '.', 'demonstrate', 'features', ',', 'certainextent', ',', 'serve', 'predictors', 'part', 'speech', ',', 'represented', 'universal', 'pos', 'tag', '(', 'das', 'petrov', ',', '2011', ')', '.keywords', ':', 'delexicalized', 'tagging', ',', 'hamdledt', '2.0', ',', 'features', 'expansion', ',', 'classifier'\"],\n",
    "               [\"LVQ_for_text_categorization_using_a_multilingual\", \"'neural', 'learning', 'used', 'e6ectiveness', 'natural', 'language', 'processing', 'tasks', '.', 'par\\x02ticularly', ',', 'widrow–ho6', 'kivinen–warmuth', 'exponentiated', 'gradient', '(', 'based', 'neurallearning', 'rules', ')', 'algorithms', 'used', 'text', 'categorization', ',', 'improving', 'results', 'obtainedby', 'well-known', 'rocchio', '’', 'algorithm', '.', 'high', 'performance', 'competitive', 'learning', 'algo\\x02rithms', ',', 'recently', 'applied', 'solve', 'information', 'retrieval', 'problems', ',', 'leads', 'us', 'use', 'thespeci=c', 'text', 'categorization', 'tasks', '.', 'paper', 'presents', 'multilingual', 'categorization', 'system', 'basedon', 'neural', 'learning', ',', 'using', 'polyglot', 'bible', 'training', 'collection', ',', 'spanish', 'english.the', 'method', 'suggest', 'based', 'using', 'lvqalgorithm', 'build', 'classi=er', 'learns', 'thetraining', 'multilingual', 'collection', '.', 'performed', 'experiments', 'four', 'algorithm', 'whichshow', 'ideas', 'describe', 'promising', 'worth', 'investigation'\"],\n",
    "               [\"Modeling_the_Hebrew_Bible_Potential\", \"'providing', 'useful', 'efficient', 'semantic', 'annotations', 'major', 'challengefor', 'knowledge', 'design', 'body', 'text', ',', 'especially', 'historical', 'documents', '.', 'inthis', 'article', ',', 'propose', 'topic', 'modeling', 'important', 'first', 'step', 'gathersemantic', 'information', 'beyond', 'lexicon', 'added', 'annotationsin', 'shebanq', '.', 'laying', 'case', 'study', ',', 'discuss', 'noise', 'andstructure', 'found', 'comparing', 'topics', 'extracted', 'within', 'different', 'distributions', ',', 'show', 'value', 'approach', ',', 'label', 'topic', 'hierarchy', '.', 'wealso', 'show', 'first', 'result', 'applying', 'approach', 'study', 'diachronic', 'varietyin', 'bible', ',', 'show', 'overall', 'topic', 'modeling', 'approach', 'resultin', 'query', 'options', 'users', 'database'\"],\n",
    "               [\"Normalizing_Historical_Orthography\", \"'historical', 'text', 'presents', 'numerous', 'challenges', 'contempo\\x02rary', 'different', 'techniques', ',', 'e.g', '.', 'information', 'retrieval', ',', 'ocrand', 'pos', 'tagging', '.', 'particular', ',', 'absence', 'consistentorthographic', 'conventions', 'historical', 'text', 'presents', 'difficul\\x02ties', 'system', 'requires', 'reference', 'fixed', 'lexiconaccessed', 'orthographic', 'form', '.', 'example', ',', 'language', 'mod\\x02eling', 'retrieval', 'engine', 'historical', 'text', 'producedby', 'ocr', 'systems', ',', 'spelling', 'words', 'often', 'differ', 'invarious', 'way', ',', 'e.g', '.', 'one', 'word', 'might', 'different', 'spellingsevolved', 'time', '.', 'important', 'aid', 'tech\\x02niques', 'rules', 'automatic', 'mapping', 'historicalwordforms', '.', 'paper', ',', 'propose', 'new', 'technique', 'tomodel', 'target', 'modern', 'language', 'means', 'recurrentneural', 'network', 'long-short', 'term', 'memory', 'architecture.because', 'network', 'recurrent', ',', 'considered', 'context', 'isnot', 'limited', 'fixed', 'size', 'especially', 'due', 'memory', 'cellswhich', 'designed', 'deal', 'long-term', 'dependencies.in', 'set', 'experiments', 'conducted', 'luther', 'bibledatabase', 'transform', 'wordforms', 'early', 'new', 'highgerman', '(', 'enhg', ')', '14th', '-', '16th', 'centuries', 'correspond\\x02ing', 'modern', 'wordforms', 'new', 'high', 'german', '(', 'nhg', ')', '.', 'wecompare', 'proposed', 'supervised', 'model', 'lstm', 'variousmethods', 'computing', 'word', 'alignments', 'using', 'statistical', ',', 'heuristic', 'models', '.', 'new', 'proposed', 'lstm', 'outperformsthe', 'three', 'state-of-the-art', 'methods', '.', 'evaluationshows', 'accuracy', 'model', 'known', 'wordforms', 'is93.90', '%', 'unknown', 'wordforms', '87.95', '%', ',', 'theaccuracy', 'existing', 'state-of-the-art', 'combined', 'approachof', 'wordlist-based', 'rule-based', 'normalization', 'models', 'is92.93', '%', 'known', '76.88', '%', 'unknown', 'tokens', '.', 'pro\\x02posed', 'lstm', 'model', 'outperforms', 'normalizing', 'modernwordform', 'historical', 'wordform', '.', 'performance', 'seentokens', '93.4', '%', ',', 'unknown', 'tokens', '89.17'\"],\n",
    "               [\"Ontology_Based_CBR_System_on_Religious\", \"'The', 'Bible', ',', 'holy', 'book', 'Christians', ',', 'contains', 'instructions', 'life', 'vastreservoir', 'information', ',', 'useful', 'life', '.', 'It', 'sufficient', 'provide', 'answer', 'queries', 'onany', 'vital', 'topic', 'life', '.', 'The', 'paper', 'focuses', '‘', 'Leadership', '’', ',', 'important', 'concept', 'inlife', 'aims', 'developing', 'Ontology', ',', 'based', 'books', 'Old', 'NewTestaments', 'Bible', '.', 'Leadership', 'one', 'vital', 'personality', 'traits', 'decide', 'howmuch', 'person', 'achieves', 'climbs', 'ladder', 'success', 'life', 'influences', 'otherpeople', 'society', 'large', '.', 'The', 'study', 'leadership', 'done', 'various', 'perspectives—historical', ',', 'sociological', ',', 'psychological', ',', 'etc', '.', 'However', ',', 'work', ',', 'approached', 'aBiblical', 'perspective', ',', 'wherein', 'Biblical', 'truths', 'used', 'diagnose', 'leadershippotential', 'person', 'recommend', 'leadership', 'scope', 'person', 'based', 'herleadership', 'abilities', '.', 'A', 'combination', 'ontological', 'approach', 'Case-Based', 'Reasoning', '(', 'CBR', ')', 'used', '.', 'The', 'Ontology', 'Web', 'Language', '(', 'OWL', ')', 'leadership', 'developed', 'andused', 'CBR', 'system', 'development', '.', 'The', 'CBR', 'system', 'developed', 'using', 'myCBR', '.', 'Any', 'usercan', 'use', 'application', 'assess', 'leadership', 'traits', ',', 'recognize', 'eligibility', 'becominga', 'leader', 'receive', 'biblical', 'recommendations', 'equip', 'future', 'leadership', '.'\"],\n",
    "               [\"Prototype_Matching_Finding_Meaning\", \"'common', 'text', 'documents', 'characterised', 'andclassified', 'keywords', 'authors', 'use', 'give', 'andname', 'text', 'characteristics', '.', 'visa', 'et', 'al', '.', ',', 'however', ',', 'developed', 'new', 'methodology', 'based', 'prototype', 'matching.the', 'prototype', 'interesting', 'document', 'part', 'anextracted', ',', 'interesting', 'text', '.', 'prototype', 'matched', 'withthe', 'existing', 'document', 'database', 'monitored', 'documentflow', '.', 'claim', 'new', 'methodology', 'capable', 'ofextracting', 'meaning', 'automatically', 'contents', 'thedocument', '.', 'verify', 'hypothesis', 'test', 'designedwith', 'bible', '.', 'two', 'different', 'translations', ',', 'one', 'englishand', 'another', 'finnish', ',', 'selected', 'test', 'text', 'material.verification', 'tests', 'included', 'search', 'ten', 'nearestbooks', 'every', 'book', 'bible', 'performed', 'de\\x02signed', 'prototype', 'version', 'software', 'application', '.', 'theinteresting', 'test', 'results', 'reported', 'paper.the', 'new', 'methodology', 'based', 'hierarchy', 'self\\x02organizing', 'maps', '(', 'som', ')', 'smart', 'encoding', 'words.the', 'words', 'text', 'document', 'encoded', '.', 'encodedwords', 'represented', 'word', 'vectors', '.', 'word', 'vectorsare', 'clustered', 'som', 'process', 'creates', 'wordmap', '.', 'words', 'text', 'document', 'replaced', 'theaddresses', 'word', 'map', '.', 'document', 'consists', 'ofa', 'sequence', 'addresses', '.', 'addresses', 'contain', 'informa\\x02tion', 'word', 'order', '.', 'document', 'considered', 'sentenceby', 'sentence', '.', 'sentence', 'vectors', 'clustered', 'som.this', 'process', 'creates', 'sentence', 'map', '.', 'sentences', 'ofthe', 'text', 'document', 'replaced', 'addresses', 'sen\\x02'\"],\n",
    "               [\"Reliable_writer_identification_in_medieval_manuscripts\", \"'In', 'field', 'manuscript', 'studies', '(', 'palaeography', 'codicology', ')', ',', 'particularly', 'interesting', 'case', 'study', 'ofhighly', 'standardized', 'handwriting', 'book', 'typologies', '.', 'In', 'cases', ',', 'analysis', 'basic', 'layout', 'features', ',', 'mainly', 'related', 'organization', 'page', 'exploitation', 'available', 'space', ',', 'may', 'helpfulfor', 'distinguishing', 'similar', 'scribal', 'hands', '.', 'In', 'framework', ',', 'defined', 'set', 'layout', 'features', 'develop', 'apattern', 'recognition', 'system', 'identifying', 'scribes', 'collaborated', 'transcription', 'single', 'medievalLatin', 'book', '.', 'We', 'also', 'experimentally', 'characterized', 'discriminative', 'power', 'considered', 'feature', 'andwe', 'verified', 'whether', 'selection', 'appropriate', 'subset', 'features', 'scribe', ',', 'specifically', 'devisedfor', 'distinguishing', 'others', ',', 'could', 'allow', 'us', 'achieve', 'better', 'results', '.', 'This', 'approach', 'allowed', 'usto', 'introduce', 'simple', 'way', 'reject', 'option', 'rejecting', 'unreliably', 'classified', 'samples', ',', 'namely', 'thosenot', 'assigned', 'scribe', 'assigned', 'scribes', '.', 'The', 'experiments', ',', 'performed', 'large', 'database', 'ofdigital', 'images', 'called', '‘', '‘', 'Avila', 'Bible', '’', '’', '–', 'giant', 'Latin', 'copy', 'whole', 'Bible', 'produced', 'XIIcentury', 'Italy', 'Spain', '–', 'confirmed', 'effectiveness', 'proposed', 'method', '.', 'Finally', ',', 'made', 'publiclyavailable', 'data', 'set', 'extracted', 'Avila', 'Bible', 'images'\"],\n",
    "               [\"Semantic_Vector_Combinations_and_the_Synoptic\", \"'paper', 'applies', 'recent', 'methods', 'involving', 'semanticvectors', 'combination', 'operations', 'traditional', 'ques\\x02tions', ',', 'including', 'discovery', 'similarities', 'differences', 'thefour', 'gospels', ',', 'relationships', 'individuals', ',', 'identification', 'ofgeopolitical', 'regions', 'leaders', 'ancient', 'world', '.', 'process', ',', 'weemploy', 'several', 'methods', 'linear', 'algebra', 'vector', 'space', 'models', ',', 'particular', 'importance', 'quantum', 'mechanics', 'andquantum', 'logic.our', 'conclusions', 'general', 'positive', ':', 'vector', 'methods', 'good', 'jobof', 'capturing', 'well-known', 'facts', 'bible', ',', 'authors', ',', 'relation\\x02ships', 'people', 'places', 'mentioned', 'bible', '.', 'morespecific', 'topic', 'quantum', 'opposed', 'approaches', ',', 'conclusionsare', 'mixed', ':', 'whole', ',', 'find', 'evidence', 'preferring', 'vec\\x02tor', 'methods', 'directly', 'associated', 'quantum', 'mechanics', 'overvector', 'methods', 'developed', 'independently', 'quantum', 'mechanics', '.', 'sug\\x02gest', 'argues', 'synthesis', 'rather', 'division', 'classicaland', 'quantum', 'models', 'information', 'processing'\"],\n",
    "               [\"Stylometric_classification_of_different_translations\", \"'quantitative', 'stylometry', 'ten', 'translations', 'bible', 'passage', 'english', ',', 'followed', 'ward', 'clustering', ',', 'produces', 'dendrogram', 'reflects', 'well-knownhistory', 'intent', 'translations', '.', 'conclude', 'quantitative', 'stylometrycombined', 'clustering', 'useful', 'tool', 'reconstructing', 'literary', 'history'\"],\n",
    "               [\"Text_Correction_Using_Approaches_Based_on_Markovian\", \"'everal', 'authors', 'reported', 'interesting', 'results', 'ob\\x02tained', 'using', 'untrained', 'randomly', 'initialized', 'recurrentpart', 'recurrent', 'neural', 'network', '(', 'rnn', ')', '.', 'instead', 'long', ',', 'difficult', 'often', 'unnecessary', 'adaptation', 'process', ',', 'dynam\\x02ics', 'based', 'fixed', 'point', 'attractors', 'rich', 'enough', 'forfurther', 'exploitation', 'tasks', '.', 'principle', 'explain\\x02ing', 'untrained', 'rnn', 'state', 'space', 'structure', 'called', 'marko\\x02vian', 'architectural', 'bias', '[', '1', ',', '8', ']', 'several', 'methods', 'usingthis', 'behavior', 'studied', '.', 'paper', 'apply', 'theseapproaches', 'correct', 'corrupted', 'symbols', 'symbol', 'se\\x02quence', '.', 'approaches', 'share', 'properties', 'vari\\x02able', 'length', 'markov', 'models', 'hence', 'experiments', 'in\\x02spired', 'paper', 'dealing', 'text', 'correction', 'thebible', 'dataset'\"],\n",
    "               [\"Text_Mining_Analysis_of_the_King_James\", \"'english', 'second', 'language', '(', 'esl', ')', 'students', 'oftenexpected', 'read', 'use', 'english', 'translations', 'bible', 'foracademic', 'worship', 'purposes', '.', 'purpose', 'studywas', 'explore', 'writing', 'style', 'king', 'james', 'versionand', 'new', 'international', 'version', 'terms', 'transla\\x02tion', '’', 'formality', ',', 'readability', ',', 'sentiment', 'using', 'quantitativetext', 'mining', 'analysis', 'tools', '.', 'results', 'indicated', 'kjv', 'usedmore', 'formal', 'language', ',', 'higher', 'grade', 'level', 'readabilityand', 'used', 'slightly', 'positive', 'wording', 'niv', '.', 'add\\x02ition', ',', 'translations', 'old', 'testament', 'much', 'morenegative', 'terms', 'sentiment', 'new', 'testament.lastly', ',', 'moderate', 'positive', 'relationship', 'found', 'betweenreadability', 'formality', 'translations', '.'\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "53dbc159",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, index=range(100, 127),columns=['Titulo', 'Paper'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "bbf4b9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                Titulo  \\\n",
      "100                  CONTEXT_BASED_MACHINE_TRANSLATION   \n",
      "101                  Deep_Transfer_Learning_for_writer   \n",
      "102               Detecting_the_Evolution_of_Semantics   \n",
      "103                  Graphics_based_intelligent_search   \n",
      "104                                 Rolling_stylometry   \n",
      "105                    Text_alignment_in_early_printed   \n",
      "106          Algorithmic_handwriting_analysis_of_Judah   \n",
      "107                 An_end_to_end_deep_learning_system   \n",
      "108            An_Experimental_Comparison_between_Deep   \n",
      "109              Biblical_intertextuality_in_a_digital   \n",
      "110    Computational_modelling_of_an_optical_character   \n",
      "111           Continuous_multilinguality_with_language   \n",
      "112   Design_and_Development_of_Part_of_Speech_Tagging   \n",
      "113    Development_of_a_Recurrent_Neural_Network_Model   \n",
      "114                   Hybrid_Part_of_Speech_Tagger_for   \n",
      "115              If_all_you_have_is_a_bit_of_the_Bible   \n",
      "116  If_You_Even_Dont_Have_a_Bit_of_Bible_Learning_...   \n",
      "117   LVQ_for_text_categorization_using_a_multilingual   \n",
      "118                Modeling_the_Hebrew_Bible_Potential   \n",
      "119                 Normalizing_Historical_Orthography   \n",
      "120             Ontology_Based_CBR_System_on_Religious   \n",
      "121                 Prototype_Matching_Finding_Meaning   \n",
      "122  Reliable_writer_identification_in_medieval_man...   \n",
      "123      Semantic_Vector_Combinations_and_the_Synoptic   \n",
      "124  Stylometric_classification_of_different_transl...   \n",
      "125  Text_Correction_Using_Approaches_Based_on_Mark...   \n",
      "126             Text_Mining_Analysis_of_the_King_James   \n",
      "\n",
      "                                                 Paper  \n",
      "100  'current', 'approaches', 'machine', 'translati...  \n",
      "101  'digital', 'paleography', ',', 'recent', 'tech...  \n",
      "102  'individual', 'differences', 'semantics', 'bel...  \n",
      "103  'paper', 'presents', 'autonomous', 'text', 'co...  \n",
      "104  'article', 'introduces', 'new', 'stylometric',...  \n",
      "105  'e', 'describe', 'technique', 'transcript', 'a...  \n",
      "106  'relationship', 'expansion', 'literacy', 'juda...  \n",
      "107  'paper', 'presents', 'end-to-end', 'system', '...  \n",
      "108  'framework', 'palaeography', ',', 'availabilit...  \n",
      "109  'centuries', 'texts', 'genres', 'connected', '...  \n",
      "110  'study', 'acquired', 'dataset', 'scanned', 'im...  \n",
      "111  'existing', 'models', 'multilingual', 'nat\u0002ura...  \n",
      "112  'paper', ',', 'report', 'design', 'part-of-spe...  \n",
      "113  'research', 'developed', 'recurrent', 'neural'...  \n",
      "114  'The', 'process', 'assigning', 'part', 'speech...  \n",
      "115  'present', 'simple', 'method', 'learning', 'pa...  \n",
      "116  'art-of-speech', '(', 'pos', ')', 'induction',...  \n",
      "117  'neural', 'learning', 'used', 'e6ectiveness', ...  \n",
      "118  'providing', 'useful', 'efficient', 'semantic'...  \n",
      "119  'historical', 'text', 'presents', 'numerous', ...  \n",
      "120  'The', 'Bible', ',', 'holy', 'book', 'Christia...  \n",
      "121  'common', 'text', 'documents', 'characterised'...  \n",
      "122  'In', 'field', 'manuscript', 'studies', '(', '...  \n",
      "123  'paper', 'applies', 'recent', 'methods', 'invo...  \n",
      "124  'quantitative', 'stylometry', 'ten', 'translat...  \n",
      "125  'everal', 'authors', 'reported', 'interesting'...  \n",
      "126  'english', 'second', 'language', '(', 'esl', '...  \n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "73b2901c",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = df['Paper'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "127f0c61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>machine translation</th>\n",
       "      <td>0.514326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>based machine</th>\n",
       "      <td>0.181526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>neural machine</th>\n",
       "      <td>0.181526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>translation nmt</th>\n",
       "      <td>0.181526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>parallel corpus</th>\n",
       "      <td>0.161301</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>achieve fluency</th>\n",
       "      <td>0.090763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>along witha</th>\n",
       "      <td>0.090763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>also questionable</th>\n",
       "      <td>0.090763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amharic language</th>\n",
       "      <td>0.090763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>amharic text</th>\n",
       "      <td>0.090763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approaches also</th>\n",
       "      <td>0.090763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approaches machine</th>\n",
       "      <td>0.090763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>approachon english</th>\n",
       "      <td>0.090763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>awareness phrase</th>\n",
       "      <td>0.090763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>based machinetranslation</th>\n",
       "      <td>0.090763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bilingual dictionary</th>\n",
       "      <td>0.090763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>built bilingual</th>\n",
       "      <td>0.090763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>case neural</th>\n",
       "      <td>0.090763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbmt parallel</th>\n",
       "      <td>0.090763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cbmt recurrent</th>\n",
       "      <td>0.090763</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                0    1    2    3    4    5    6    7    8   \\\n",
       "machine translation       0.514326  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "based machine             0.181526  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "neural machine            0.181526  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "translation nmt           0.181526  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "parallel corpus           0.161301  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "achieve fluency           0.090763  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "along witha               0.090763  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "also questionable         0.090763  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "amharic language          0.090763  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "amharic text              0.090763  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "approaches also           0.090763  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "approaches machine        0.090763  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "approachon english        0.090763  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "awareness phrase          0.090763  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "based machinetranslation  0.090763  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "bilingual dictionary      0.090763  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "built bilingual           0.090763  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "case neural               0.090763  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "cbmt parallel             0.090763  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "cbmt recurrent            0.090763  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "                           9   ...   17   18   19   20   21   22   23   24  \\\n",
       "machine translation       0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "based machine             0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "neural machine            0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "translation nmt           0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "parallel corpus           0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "achieve fluency           0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "along witha               0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "also questionable         0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "amharic language          0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "amharic text              0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "approaches also           0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "approaches machine        0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "approachon english        0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "awareness phrase          0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "based machinetranslation  0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "bilingual dictionary      0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "built bilingual           0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "case neural               0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "cbmt parallel             0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "cbmt recurrent            0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0   \n",
       "\n",
       "                           25   26  \n",
       "machine translation       0.0  0.0  \n",
       "based machine             0.0  0.0  \n",
       "neural machine            0.0  0.0  \n",
       "translation nmt           0.0  0.0  \n",
       "parallel corpus           0.0  0.0  \n",
       "achieve fluency           0.0  0.0  \n",
       "along witha               0.0  0.0  \n",
       "also questionable         0.0  0.0  \n",
       "amharic language          0.0  0.0  \n",
       "amharic text              0.0  0.0  \n",
       "approaches also           0.0  0.0  \n",
       "approaches machine        0.0  0.0  \n",
       "approachon english        0.0  0.0  \n",
       "awareness phrase          0.0  0.0  \n",
       "based machinetranslation  0.0  0.0  \n",
       "bilingual dictionary      0.0  0.0  \n",
       "built bilingual           0.0  0.0  \n",
       "case neural               0.0  0.0  \n",
       "cbmt parallel             0.0  0.0  \n",
       "cbmt recurrent            0.0  0.0  \n",
       "\n",
       "[20 rows x 27 columns]"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(2,2))\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "tf_idf = pd.DataFrame(data = X.toarray(), columns=vectorizer.get_feature_names())\n",
    "\n",
    "final_df = tf_idf\n",
    "\n",
    "print(\"{} rows\".format(final_df.shape[0]))\n",
    "final_df.T.nlargest(20, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4307cb03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2abd5b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
